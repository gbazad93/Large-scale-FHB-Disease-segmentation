# Large-scale-FHB-Disease-segmentation
A Large-scale Wheat-FHB disease dataset for segmentation and classification tasks. We proposed a a dual-attention bi-directional Conv-LSTM U-Net as a baseline method for the segmentation task.


## Abstract

Fusarium head blight (FHB) is a serious disease that affects wheat and barley grown. South Dakota alone suffers $20 million in losses annually from FHB. The early detection of FHB disease will improve the efficiency, accuracy, and capability of FHB resistance screening in wheat, durum, and barley breeding. Such diseases can be automatically or semi-automatically diagnosed using supervised machine learning algorithms and image processing techniques. Several methods have been proposed to plant disease detection. Among these methods, Convolutional neural networks (CNNs) are an effective method to automatically identify these diseases. Recently, different approaches have been proposed to boost CNNsâ€™ performance. Most of these methods, however, suffer from a lack of a mechanism that allows them to integrate global and local contextual information adaptively to extract hidden patterns inside the input images. In this regard, we propose the multi-scale attention U-Net. Our main goal is to improve U-net by introducing an attention mechanism at the network bottleneck. In the proposed method, to adaptively emphasize the importance of both spatial and channel dimensions we propose to include the dual attention mechanism. To this end, using the global information of each channel we learn the scaling coefficient to improve the object learning process. Furthermore, by learning the self-attention map we impose the spatial attention map on the feature space to adaptively emphasize the important regions. We apply the suggested dual attention in a multi-scale fashion to encourage a multi-scale learning process. To train the proposed network, we prepared 12,000 images in an SDSU wheat field in Volga, SD. In addition to covering healthy and diseased plants, these images include different stages of the disease. A group of plant disease experts annotated the prepared images so that these images can be used to train the deep model. Using these images, the model was trained to separate the diseased areas from the healthy areas by receiving the input images. Furthermore, for the first time to the best of our knowledge, we determined the stage of disease based on the segmentation results of the model. As a result, we are able to detect the diseased areas of the plants, as well as determine the disease stage. In the test phase, an automatic plant scanning robot was developed and used to evaluate the performance of the proposed model. The experimental results demonstrate that our approach works well in estimating crop contamination accurately.
